{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFLOW WITH GEN AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the important libraries\n",
    "\n",
    "1. the dagshub offers mlflow\n",
    "2. Use dotenv to load env variables\n",
    "3. Import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import dagshub\n",
    "from mlflow.metrics.genai import EvaluationExample, answer_similarity,make_genai_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]= os.getenv(\"API_KEY_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dagshub\n",
    "1. need to put the token file of dagshub in the user directory\n",
    "2. set the mlflow tracking_uri based on the repoName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository MLflow doesn't exist, creating it under current user.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository MLflow doesn't exist, creating it under current user.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"mohannadrateb/MLflow\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"mohannadrateb/MLflow\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository mohannadrateb/MLflow initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository mohannadrateb/MLflow initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dagshub.init(repo_owner='mohannadrateb',repo_name=\"MLflow\", mlflow=True)\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/mohannadrateb/MLflow.mlflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataframe for evaluation\n",
    "1. There are two columns in it. The inputs which will represent the input given to the model and the ground truth is the answers to theses\n",
    "inputs which we will compare with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"How does useEffect() work?\",\n",
    "            \"What does the static keyword in a function mean?\",\n",
    "            \"What does the 'finally' block in Python do?\",\n",
    "            \"What is the difference between multiprocessing and multithreading?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we’ll refer to it as our “effect”), and call it later after performing the DOM updates.\",\n",
    "            \"Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.\",\n",
    "            \"'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.\",\n",
    "            \"Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mlflow guidelines\n",
    "1. Using the mlflow.set_experiment(< experiment_name >) A new experiment is registered in mlflow which we can later access in MLFLOW UI\n",
    "2. mlflow.start_run(), to start running the experiment\n",
    "3. Define the model as well as the their inputs\n",
    "4. mlflow.openai.log_model --> to  wrap the model in MLFLOW AND  can compare it later,use an openai model and to keep track of it -->.log_model()\n",
    "5. use mlflow.evaluate(), to get results of different metrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohannadrateb/Desktop/portfolio/Port_projects/Data-Science-Eng-Portfolio/MLFLOW/LLM_Evaluate/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 14.30it/s]\n",
      "2024/07/12 15:16:57 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/07/12 15:17:04 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/07/12 15:17:04 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:17:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/07/12 15:17:04 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:17:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/07/12 15:17:04 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:17:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "2024/07/12 15:17:04 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:17:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/07/12 15:17:04 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:17:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/07/12 15:17:04 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:17:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match/v1': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"LLM Evaluation\")\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Answer the following question in two sentences\"\n",
    "    basic_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.chat.completions,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        targets=\"ground_truth\",  # specify which column corresponds to the expected output\n",
    "        model_type=\"question-answering\",  # model type indicates which metrics are relevant for this task\n",
    "        evaluators=\"default\",\n",
    "    )\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results from model.eval()\n",
    "1. has an attrbuite of tables with the name [\"eval_results_table\"], which contain the results of the model as well as the column of ground truth to compare with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does useEffect() work?</td>\n",
       "      <td>The useEffect() hook tells React that your com...</td>\n",
       "      <td>useEffect() is a hook provided by React that a...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does the static keyword in a function mean?</td>\n",
       "      <td>Static members belongs to the class, rather th...</td>\n",
       "      <td>The static keyword in a function means that th...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the 'finally' block in Python do?</td>\n",
       "      <td>'Finally' defines a block of code to run when ...</td>\n",
       "      <td>The 'finally' block in Python is used to defin...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between multiprocessing...</td>\n",
       "      <td>Multithreading refers to the ability of a proc...</td>\n",
       "      <td>Multiprocessing involves utilizing multiple pr...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0                         How does useEffect() work?   \n",
       "1   What does the static keyword in a function mean?   \n",
       "2        What does the 'finally' block in Python do?   \n",
       "3  What is the difference between multiprocessing...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  The useEffect() hook tells React that your com...   \n",
       "1  Static members belongs to the class, rather th...   \n",
       "2  'Finally' defines a block of code to run when ...   \n",
       "3  Multithreading refers to the ability of a proc...   \n",
       "\n",
       "                                             outputs  token_count  \n",
       "0  useEffect() is a hook provided by React that a...           42  \n",
       "1  The static keyword in a function means that th...           48  \n",
       "2  The 'finally' block in Python is used to defin...           47  \n",
       "3  Multiprocessing involves utilizing multiple pr...           36  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of custom evaluation example.\n",
    "1. using mlflow.metrics.genai give input, output, score and justification for that score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example to describe what answer_similarity means like for this problem.\n",
    "example = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow is an open-source platform for managing machine \"\n",
    "    \"learning workflows, including experiment tracking, model packaging, \"\n",
    "    \"versioning, and deployment, simplifying the ML lifecycle.\",\n",
    "    score=4,\n",
    "    justification=\"The definition effectively explains what MLflow is \"\n",
    "    \"its purpose, and its developer. It could be more concise for a 5-score.\",\n",
    "    grading_context={\n",
    "        \"targets\": \"MLflow is an open-source platform for managing \"\n",
    "        \"the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, \"\n",
    "        \"a company that specializes in big data and machine learning solutions. MLflow is \"\n",
    "        \"designed to address the challenges that data scientists and machine learning \"\n",
    "        \"engineers face when developing, training, and deploying machine learning models.\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use this example in the answer_similarity_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=answer_similarity, greater_is_better=True, long_name=answer_similarity, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's answer_similarity based on the rubric\n",
      "justification: Your reasoning about the model's answer_similarity score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called answer_similarity based on the input and output.\n",
      "A definition of answer_similarity and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer similarity is evaluated on the degree of semantic similarity of the provided output to the provided targets, which is the ground truth. Scores can be assigned based on the gradual similarity in meaning and description to the provided targets, where a higher score indicates greater alignment between the provided output and provided targets.\n",
      "\n",
      "Grading rubric:\n",
      "Answer similarity: Below are the details for different scores:\n",
      "- Score 1: The output has little to no semantic similarity to the provided targets.\n",
      "- Score 2: The output displays partial semantic similarity to the provided targets on some aspects.\n",
      "- Score 3: The output has moderate semantic similarity to the provided targets.\n",
      "- Score 4: The output aligns with the provided targets in most aspects and has substantial semantic similarity.\n",
      "- Score 5: The output closely aligns with the provided targets in all significant aspects.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Output:\n",
      "MLflow is an open-source platform for managing machine learning workflows, including experiment tracking, model packaging, versioning, and deployment, simplifying the ML lifecycle.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: targets\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "Example score: 4\n",
      "Example justification: The definition effectively explains what MLflow is its purpose, and its developer. It could be more concise for a 5-score.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's answer_similarity based on the rubric\n",
      "justification: Your reasoning about the model's answer_similarity score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "# Construct the metric using OpenAI GPT-4 as the judge\n",
    "answer_similarity_metric = answer_similarity(model=\"openai:/gpt-4\", examples=[example])\n",
    "print(answer_similarity_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can define extra metrcies in the mlflow.evalutate to be added to our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 15.15it/s]\n",
      "2024/07/12 15:43:49 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/07/12 15:43:56 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/07/12 15:43:56 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:43:56 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/07/12 15:43:56 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:43:56 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/07/12 15:43:56 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:43:56 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.06s/it]\n",
      "2024/07/12 15:44:00 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:44:00 WARNING mlflow.models.evaluation.default_evaluator: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2024/07/12 15:44:00 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:44:00 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/07/12 15:44:00 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:44:00 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/07/12 15:44:00 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:44:00 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\n",
      "2024/07/12 15:44:04 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:44:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'latency/mean': 1.663176715373993,\n",
       " 'latency/variance': 0.01992190785308523,\n",
       " 'latency/p90': 1.811255145072937,\n",
       " 'exact_match/v1': 0.0,\n",
       " 'answer_similarity/v1/mean': 3.75,\n",
       " 'answer_similarity/v1/variance': 1.1875,\n",
       " 'answer_similarity/v1/p90': 4.7}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        targets=\"ground_truth\",\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[answer_similarity_metric,mlflow.metrics.toxicity(), mlflow.metrics.latency()],  # use the answer similarity metric created above\n",
    "    )\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>latency</th>\n",
       "      <th>token_count</th>\n",
       "      <th>answer_similarity/v1/score</th>\n",
       "      <th>answer_similarity/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does useEffect() work?</td>\n",
       "      <td>The useEffect() hook tells React that your com...</td>\n",
       "      <td>useEffect() is a hook in React that allows you...</td>\n",
       "      <td>1.468170</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>The output effectively explains what useEffect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does the static keyword in a function mean?</td>\n",
       "      <td>Static members belongs to the class, rather th...</td>\n",
       "      <td>The static keyword in a function means that th...</td>\n",
       "      <td>1.601177</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>The output provides some information about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the 'finally' block in Python do?</td>\n",
       "      <td>'Finally' defines a block of code to run when ...</td>\n",
       "      <td>The 'finally' block in Python is used to execu...</td>\n",
       "      <td>1.742742</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>The model's output closely aligns with the pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between multiprocessing...</td>\n",
       "      <td>Multithreading refers to the ability of a proc...</td>\n",
       "      <td>Multiprocessing involves executing multiple pr...</td>\n",
       "      <td>1.840618</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>The output accurately describes the concepts o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0                         How does useEffect() work?   \n",
       "1   What does the static keyword in a function mean?   \n",
       "2        What does the 'finally' block in Python do?   \n",
       "3  What is the difference between multiprocessing...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  The useEffect() hook tells React that your com...   \n",
       "1  Static members belongs to the class, rather th...   \n",
       "2  'Finally' defines a block of code to run when ...   \n",
       "3  Multithreading refers to the ability of a proc...   \n",
       "\n",
       "                                             outputs   latency  token_count  \\\n",
       "0  useEffect() is a hook in React that allows you...  1.468170           45   \n",
       "1  The static keyword in a function means that th...  1.601177           47   \n",
       "2  The 'finally' block in Python is used to execu...  1.742742           49   \n",
       "3  Multiprocessing involves executing multiple pr...  1.840618           52   \n",
       "\n",
       "   answer_similarity/v1/score  \\\n",
       "0                           4   \n",
       "1                           2   \n",
       "2                           5   \n",
       "3                           4   \n",
       "\n",
       "                  answer_similarity/v1/justification  \n",
       "0  The output effectively explains what useEffect...  \n",
       "1  The output provides some information about the...  \n",
       "2  The model's output closely aligns with the pro...  \n",
       "3  The output accurately describes the concepts o...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom genai metric\n",
    "1. we will need to give the grading guidelines  in the \"grading_prompt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=professionalism, greater_is_better=True, long_name=professionalism, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's professionalism based on the rubric\n",
      "justification: Your reasoning about the model's professionalism score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called professionalism based on the input and output.\n",
      "A definition of professionalism and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language\n",
      "\n",
      "Grading rubric:\n",
      "Professionalism: If the answer is written using a professional tone, below are the details for different scores: - Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. - Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. - Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks. \n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Input:\n",
      "What is MLflow?\n",
      "\n",
      "Example Output:\n",
      "MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It's like your Swiss Army knife for machine learning!\n",
      "\n",
      "Example score: 2\n",
      "Example justification: The response is written in a casual tone. It uses contractions, filler words such as 'like', and exclamation points, which make it sound less professional. \n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's professionalism based on the rubric\n",
      "justification: Your reasoning about the model's professionalism score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "professionalism_metric = make_genai_metric(\n",
    "    name=\"professionalism\",\n",
    "    definition=(\n",
    "        \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Professionalism: If the answer is written using a professional tone, below \"\n",
    "        \"are the details for different scores: \"\n",
    "        \"- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.\"\n",
    "        \"- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.\"\n",
    "        \"- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \"\n",
    "        \"- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. \"\n",
    "        \"- Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks. \"\n",
    "    ),\n",
    "    examples=[\n",
    "        EvaluationExample(\n",
    "            input=\"What is MLflow?\",\n",
    "            output=(\n",
    "                \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It's like your Swiss Army knife for machine learning!\"\n",
    "            ),\n",
    "            score=2,\n",
    "            justification=(\n",
    "                \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and exclamation points, which make it sound less professional. \"\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    version=\"v1\",\n",
    "    model=\"openai:/gpt-4\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    grading_context_columns=[],\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "print(professionalism_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 14.71it/s]\n",
      "2024/07/12 15:50:32 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/07/12 15:50:38 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/07/12 15:50:38 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:50:38 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/07/12 15:50:38 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:50:38 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/07/12 15:50:38 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:50:38 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "2024/07/12 15:50:38 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
      "2024/07/12 15:50:40 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:50:40 WARNING mlflow.models.evaluation.default_evaluator: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2024/07/12 15:50:40 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:50:40 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/07/12 15:50:40 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:50:40 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/07/12 15:50:40 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:50:40 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "2024/07/12 15:50:40 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n",
      "100%|██████████| 4/4 [00:39<00:00,  9.88s/it]\n",
      "2024/07/12 15:51:20 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:51:20 WARNING mlflow.models.evaluation.default_evaluator: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latency/mean': 1.5089607238769531, 'latency/variance': 0.07772480408246452, 'latency/p90': 1.827316975593567, 'professionalism/v1/mean': 4.0, 'professionalism/v1/variance': 0.0, 'professionalism/v1/p90': 4.0}\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[professionalism_metric,mlflow.metrics.toxicity(), mlflow.metrics.latency()],  # use the professionalism metric we created above\n",
    "    )\n",
    "print(results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>latency</th>\n",
       "      <th>token_count</th>\n",
       "      <th>professionalism/v1/score</th>\n",
       "      <th>professionalism/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does useEffect() work?</td>\n",
       "      <td>The useEffect() hook tells React that your com...</td>\n",
       "      <td>useEffect() is a hook in React that allows you...</td>\n",
       "      <td>1.227306</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does the static keyword in a function mean?</td>\n",
       "      <td>Static members belongs to the class, rather th...</td>\n",
       "      <td>The static keyword in a function means that th...</td>\n",
       "      <td>1.313091</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the 'finally' block in Python do?</td>\n",
       "      <td>'Finally' defines a block of code to run when ...</td>\n",
       "      <td>The 'finally' block in Python is used to defin...</td>\n",
       "      <td>1.548738</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between multiprocessing...</td>\n",
       "      <td>Multithreading refers to the ability of a proc...</td>\n",
       "      <td>Multiprocessing involves running multiple proc...</td>\n",
       "      <td>1.946708</td>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0                         How does useEffect() work?   \n",
       "1   What does the static keyword in a function mean?   \n",
       "2        What does the 'finally' block in Python do?   \n",
       "3  What is the difference between multiprocessing...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  The useEffect() hook tells React that your com...   \n",
       "1  Static members belongs to the class, rather th...   \n",
       "2  'Finally' defines a block of code to run when ...   \n",
       "3  Multithreading refers to the ability of a proc...   \n",
       "\n",
       "                                             outputs   latency  token_count  \\\n",
       "0  useEffect() is a hook in React that allows you...  1.227306           46   \n",
       "1  The static keyword in a function means that th...  1.313091           46   \n",
       "2  The 'finally' block in Python is used to defin...  1.548738           47   \n",
       "3  Multiprocessing involves running multiple proc...  1.946708           68   \n",
       "\n",
       "   professionalism/v1/score                   professionalism/v1/justification  \n",
       "0                         4  The language used in the response is formal an...  \n",
       "1                         4  The language used in the response is formal an...  \n",
       "2                         4  The language used in the response is formal an...  \n",
       "3                         4  The language used in the response is formal an...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 13.86it/s]\n",
      "2024/07/12 15:53:52 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/07/12 15:54:01 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/07/12 15:54:01 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:54:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/07/12 15:54:01 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:54:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/07/12 15:54:01 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:54:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "2024/07/12 15:54:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n",
      "2024/07/12 15:54:04 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:54:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2024/07/12 15:54:04 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:54:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/07/12 15:54:04 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:54:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/07/12 15:54:04 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/07/12 15:54:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "2024/07/12 15:54:04 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.20it/s]\n",
      "2024/07/12 15:54:07 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/07/12 15:54:07 WARNING mlflow.models.evaluation.default_evaluator: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latency/mean': 2.177800953388214, 'latency/variance': 0.3162884970014481, 'latency/p90': 2.7973836898803714, 'professionalism/v1/mean': 4.0, 'professionalism/v1/variance': 0.0, 'professionalism/v1/p90': 4.0}\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Answer the following question using extreme formality.\"\n",
    "    professional_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.chat.completions,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        professional_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[professionalism_metric,mlflow.metrics.toxicity(), mlflow.metrics.latency()],\n",
    "    )\n",
    "print(results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>latency</th>\n",
       "      <th>token_count</th>\n",
       "      <th>professionalism/v1/score</th>\n",
       "      <th>professionalism/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does useEffect() work?</td>\n",
       "      <td>The useEffect() hook tells React that your com...</td>\n",
       "      <td>The useEffect() hook is a function provided by...</td>\n",
       "      <td>3.069404</td>\n",
       "      <td>159</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does the static keyword in a function mean?</td>\n",
       "      <td>Static members belongs to the class, rather th...</td>\n",
       "      <td>The static keyword within a function declarati...</td>\n",
       "      <td>1.946770</td>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the 'finally' block in Python do?</td>\n",
       "      <td>'Finally' defines a block of code to run when ...</td>\n",
       "      <td>The 'finally' block in Python is a crucial con...</td>\n",
       "      <td>1.532360</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between multiprocessing...</td>\n",
       "      <td>Multithreading refers to the ability of a proc...</td>\n",
       "      <td>The distinction between multiprocessing and mu...</td>\n",
       "      <td>2.162670</td>\n",
       "      <td>102</td>\n",
       "      <td>4</td>\n",
       "      <td>The language used in the response is formal an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0                         How does useEffect() work?   \n",
       "1   What does the static keyword in a function mean?   \n",
       "2        What does the 'finally' block in Python do?   \n",
       "3  What is the difference between multiprocessing...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  The useEffect() hook tells React that your com...   \n",
       "1  Static members belongs to the class, rather th...   \n",
       "2  'Finally' defines a block of code to run when ...   \n",
       "3  Multithreading refers to the ability of a proc...   \n",
       "\n",
       "                                             outputs   latency  token_count  \\\n",
       "0  The useEffect() hook is a function provided by...  3.069404          159   \n",
       "1  The static keyword within a function declarati...  1.946770          101   \n",
       "2  The 'finally' block in Python is a crucial con...  1.532360           66   \n",
       "3  The distinction between multiprocessing and mu...  2.162670          102   \n",
       "\n",
       "   professionalism/v1/score                   professionalism/v1/justification  \n",
       "0                         4  The language used in the response is formal an...  \n",
       "1                         4  The language used in the response is formal an...  \n",
       "2                         4  The language used in the response is formal an...  \n",
       "3                         4  The language used in the response is formal an...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
